{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "252cefe1",
   "metadata": {},
   "source": [
    "## 프로젝트: 멋진 작사가 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67123f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import glob  #glob 모듈의 glob 함수는 사용자가 제시한 조건에 맞는 파일명을 리스트 형식으로 반환한다\n",
    "import tensorflow\n",
    "import os, re \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17380f5c",
   "metadata": {},
   "source": [
    "### Step 1. 데이터 다운로드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7f9239",
   "metadata": {},
   "source": [
    " - 이미 실습(데이터 다듬기)에서 Cloud shell에 심볼릭 링크로 ~/aiffel/lyricist/data를 생성, ~/aiffel/lyricist/data/lyrics에 데이터 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179f83ca",
   "metadata": {},
   "source": [
    "### Step 2. 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54a7c56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n"
     ]
    }
   ],
   "source": [
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*' #os.getenv(x)함수는 환경 변수x의 값을 포함하는 문자열 변수를 반환합니다. txt_file_path 에 \"/root/aiffel/lyricist/data/lyrics/*\" 저장\n",
    "txt_list = glob.glob(txt_file_path) #txt_file_path 경로에 있는 모든 파일명을 리스트 형식으로 txt_list 에 할당\n",
    "\n",
    "raw_corpus = [] \n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines() #read() : 파일 전체의 내용을 하나의 문자열로 읽어온다. , splitlines()  : 여러라인으로 구분되어 있는 문자열을 한라인씩 분리하여 리스트로 반환\n",
    "        raw_corpus.extend(raw) # extend() : 리스트함수로 추가적인 내용을 연장 한다.\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e043a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '',\n",
       " '[Spoken Intro:]',\n",
       " 'You ever want something ',\n",
       " \"that you know you shouldn't have \",\n",
       " \"The more you know you shouldn't have it, \",\n",
       " 'The more you want it ',\n",
       " 'And then one day you get it, ',\n",
       " \"It's so good too \",\n",
       " \"But it's just like my girl \",\n",
       " \"When she's around me \",\n",
       " 'I just feel so good, so good ',\n",
       " 'But right now I just feel cold, so cold ',\n",
       " 'Right down to my bones ',\n",
       " \"'Cause ooh... \"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_corpus[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b067e412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You ever want something \n",
      "that you know you shouldn't have \n",
      "The more you know you shouldn't have it, \n",
      "The more you want it \n",
      "And then one day you get it, \n",
      "It's so good too \n",
      "But it's just like my girl \n"
     ]
    }
   ],
   "source": [
    "# enumerate() 함수를 이용하여 raw_corpus list 내에 저장된 문장과 그 문장의 인덱스를 반환 (인덱스, 문장 순)\n",
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n",
    "    if sentence[-1] == \"]\": continue # 문장의 끝이 ] 인 문장은 건너뜁니다.\n",
    "    if sentence[0] == \"[\": continue # 문장의 시작이 [ 인 문장은 건너뜁니다.\n",
    "\n",
    "    if idx > 9: break   # 일단 문장 10개만 확인해 볼 겁니다.\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69ce315",
   "metadata": {},
   "source": [
    "### Step 3. 데이터 정제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b13280",
   "metadata": {},
   "source": [
    "- 문장을 토큰화 했을 때 토큰의 개수가 15개를 넘어가는 문장을 학습 데이터에서 제외하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbac3438",
   "metadata": {},
   "outputs": [],
   "source": [
    "#토큰화\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 특수문자 양쪽에 공백을 넣고\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 여러개의 공백은 하나의 공백으로 바꿉니다\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
    "    sentence = sentence.strip() # 다시 양쪽 공백을 지웁니다\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ddbcb1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> spoken intro <end>',\n",
       " '<start> you ever want something <end>',\n",
       " '<start> that you know you shouldn t have <end>',\n",
       " '<start> the more you know you shouldn t have it , <end>',\n",
       " '<start> the more you want it <end>',\n",
       " '<start> and then one day you get it , <end>',\n",
       " '<start> it s so good too <end>',\n",
       " '<start> but it s just like my girl <end>',\n",
       " '<start> when she s around me <end>',\n",
       " '<start> i just feel so good , so good <end>']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#정제 데이터 구축하기\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue #길이 0\n",
    "    if len(sentence.split()) >= 13: continue  #15개 이하(start,end포함)\n",
    "    \n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "    \n",
    "    \n",
    "corpus[:10] #정제결과 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800a0ea1",
   "metadata": {},
   "source": [
    "### Step 4. 평가 데이터셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20c899d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2 2687 2526 ...    0    0    0]\n",
      " [   2    7  161 ...    0    0    0]\n",
      " [   2   17    7 ...    0    0    0]\n",
      " ...\n",
      " [   2   44    6 ...    0    0    0]\n",
      " [   2   31    7 ...    0    0    0]\n",
      " [   2  305    1 ...    0    0    0]] \n",
      " <keras_preprocessing.text.Tokenizer object at 0x7f15007741f0>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(158876, 158876)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    # Tokenizer 패키지를 생성합니다.\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000,  # 단어장의 크기를 설정합니다. (권장 12,000 이상)      \n",
    "        filters=' ',      # 별도의 전처리 로직           \n",
    "        oov_token=\"<unk>\" # out-of-vocabulary, 사전에 없었던 단어            \n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)   # corpus로부터 Tokenizer가 사전을 구축하는 함수       \n",
    "\n",
    "    # 이후 tokenizer를 활용하여 모델에 입력할 데이터셋을 구축하게 됩니다.\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환합니다.\n",
    "\n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기 위한 padding  메소드를 제공합니다.   \n",
    "    # maxlen는 시퀀스 길이를 뜻합니다. 지정하지 않을 때는 None이 디폴트값입니다.       \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, maxlen=15, padding='post')  \n",
    "\n",
    "    print(tensor,'\\n', tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)\n",
    "\n",
    "len(tensor), len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "042d0b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2 2687 2526    3    0    0    0    0    0    0    0    0]\n",
      " [   2    7  161   65  193    3    0    0    0    0    0    0]\n",
      " [   2   17    7   36    7 1509   15   74    3    0    0    0]\n",
      " [   2    6   99    7   36    7 1509   15   74   11    4    3]\n",
      " [   2    6   99    7   65   11    3    0    0    0    0    0]\n",
      " [   2    8   94   60  120    7   45   11    4    3    0    0]\n",
      " [   2   11   16   31  109  101    3    0    0    0    0    0]\n",
      " [   2   34   11   16   32   24   13   82    3    0    0    0]\n",
      " [   2   47   46   16  136   12    3    0    0    0    0    0]\n",
      " [   2    5   32  106   31  109    4   31  109    3    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(tensor[:10, :12])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1511280b",
   "metadata": {},
   "source": [
    "모든 데이터가 2로 시작하고, 3으로 끝나는 것을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c170f3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "268440c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2 2687 2526    3    0    0    0    0    0    0    0    0    0    0]\n",
      "[2687 2526    3    0    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# 소스 문장 : tensor에서 마지막 토큰(<end> or <pad>)을 제외합니다. : <pad>일 가능성이 높습니다. \n",
    "src_input = tensor[:, :-1]  \n",
    "# 타겟 문장 : tensor에서 <start>를 잘라내 생성합니다.\n",
    "tgt_input = tensor[:, 1:]   \n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0b4e556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#데이터셋 객체 생성\n",
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82c79549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (127100, 14)\n",
      "Target Train: (127100, 14)\n"
     ]
    }
   ],
   "source": [
    "#총 데이터의 20% 를 평가 데이터셋\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, \n",
    "                                                          tgt_input,\n",
    "                                                          test_size=0.2,\n",
    "                                                          shuffle=True, \n",
    "                                                          random_state=34)\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9866402",
   "metadata": {},
   "source": [
    "### Step 5. 인공지능 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02dc5ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        # return_sequences = 마지막 출력을 반환할지에 대한 여부 \n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 1024\n",
    "hidden_size = 2048\n",
    "model = TextGenerator(VOCAB_SIZE, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c72f1b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
       "array([[[-2.23811454e-04,  8.34885850e-06, -3.18835664e-04, ...,\n",
       "         -1.77067595e-05, -1.10747904e-04,  1.35690134e-04],\n",
       "        [-1.14840605e-05,  1.20262317e-04, -5.28747390e-04, ...,\n",
       "          7.71697796e-06,  2.83412919e-05, -1.92902357e-04],\n",
       "        [-7.31919442e-07, -5.49882883e-04, -6.35609089e-04, ...,\n",
       "         -3.24652239e-04, -1.94552020e-04, -3.51457769e-04],\n",
       "        ...,\n",
       "        [ 5.92070166e-04, -7.40265168e-05,  6.59751182e-04, ...,\n",
       "         -4.90202510e-04, -2.11748667e-03, -2.22471100e-03],\n",
       "        [ 4.51452885e-04, -1.01067410e-04,  8.15091829e-04, ...,\n",
       "         -1.81786483e-04, -2.30493932e-03, -2.44311499e-03],\n",
       "        [-2.95284735e-06, -3.42413463e-04,  1.14247086e-03, ...,\n",
       "          4.59505463e-05, -2.37151841e-03, -2.30703293e-03]],\n",
       "\n",
       "       [[-2.23811454e-04,  8.34885850e-06, -3.18835664e-04, ...,\n",
       "         -1.77067595e-05, -1.10747904e-04,  1.35690134e-04],\n",
       "        [-1.14840605e-05,  1.20262317e-04, -5.28747390e-04, ...,\n",
       "          7.71697796e-06,  2.83412919e-05, -1.92902357e-04],\n",
       "        [-1.67860184e-04, -2.14600324e-04, -1.09107280e-03, ...,\n",
       "         -2.90737516e-04,  2.45890900e-04, -6.14264864e-04],\n",
       "        ...,\n",
       "        [-1.38529181e-03, -1.73935283e-03, -6.92504254e-05, ...,\n",
       "          5.93068893e-04, -1.74152979e-03, -5.33832237e-04],\n",
       "        [-1.96113321e-03, -2.36884947e-03,  6.05202978e-04, ...,\n",
       "          3.99424956e-04, -2.03775917e-03, -1.42006000e-04],\n",
       "        [-2.47873785e-03, -3.01655452e-03,  1.23389100e-03, ...,\n",
       "          1.81251700e-04, -2.27307063e-03,  2.10290949e-04]],\n",
       "\n",
       "       [[-2.23811454e-04,  8.34885850e-06, -3.18835664e-04, ...,\n",
       "         -1.77067595e-05, -1.10747904e-04,  1.35690134e-04],\n",
       "        [-3.04125802e-04,  1.53388872e-04, -6.91167719e-04, ...,\n",
       "         -8.98752944e-04, -3.20351071e-04,  2.05107484e-04],\n",
       "        [-4.76799381e-04,  8.35848216e-04, -9.84805869e-04, ...,\n",
       "         -9.83220874e-04, -2.05548771e-04,  5.75986633e-04],\n",
       "        ...,\n",
       "        [-9.22762672e-04, -1.67244056e-03,  8.12161772e-04, ...,\n",
       "         -6.83386344e-04, -2.22272309e-03,  3.25933943e-04],\n",
       "        [-1.70067057e-03, -2.41386658e-03,  1.29198388e-03, ...,\n",
       "         -7.50005885e-04, -2.36954098e-03,  4.55383182e-04],\n",
       "        [-2.39889254e-03, -3.14722396e-03,  1.75728707e-03, ...,\n",
       "         -8.59605556e-04, -2.47106957e-03,  5.95077698e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-2.23811454e-04,  8.34885850e-06, -3.18835664e-04, ...,\n",
       "         -1.77067595e-05, -1.10747904e-04,  1.35690134e-04],\n",
       "        [-4.79926937e-04, -4.08494030e-04, -4.64574376e-04, ...,\n",
       "         -1.04027873e-04, -2.27287033e-04,  3.49456590e-04],\n",
       "        [-5.59466484e-04, -1.03597960e-03, -6.34835975e-04, ...,\n",
       "         -4.51342377e-04, -5.20274974e-04,  3.77236458e-04],\n",
       "        ...,\n",
       "        [-3.09510157e-03, -1.99231203e-03,  1.24602567e-03, ...,\n",
       "          3.44326079e-04, -2.88897101e-03,  6.47561275e-04],\n",
       "        [-3.47404741e-03, -2.66941800e-03,  1.64528110e-03, ...,\n",
       "          1.70651867e-04, -2.89941183e-03,  8.41902569e-04],\n",
       "        [-3.80737032e-03, -3.34740686e-03,  2.03022920e-03, ...,\n",
       "         -3.88058543e-05, -2.88510951e-03,  1.02289452e-03]],\n",
       "\n",
       "       [[-2.23811454e-04,  8.34885850e-06, -3.18835664e-04, ...,\n",
       "         -1.77067595e-05, -1.10747904e-04,  1.35690134e-04],\n",
       "        [-3.85593186e-04, -2.45815667e-04, -5.12629864e-04, ...,\n",
       "         -1.06378109e-04,  4.36621165e-04,  3.38607177e-04],\n",
       "        [ 5.90832751e-05,  1.30517117e-04, -4.76577115e-04, ...,\n",
       "         -3.79868899e-04,  9.40856175e-04,  2.99788808e-04],\n",
       "        ...,\n",
       "        [-2.74454313e-03, -2.47167191e-03,  1.51671388e-03, ...,\n",
       "         -1.06189447e-03, -1.74620620e-03,  7.87673285e-04],\n",
       "        [-3.20760277e-03, -3.17284930e-03,  1.96509785e-03, ...,\n",
       "         -1.16542401e-03, -1.92887837e-03,  1.06458250e-03],\n",
       "        [-3.59570188e-03, -3.83241638e-03,  2.37577315e-03, ...,\n",
       "         -1.26816833e-03, -2.07785796e-03,  1.30418129e-03]],\n",
       "\n",
       "       [[-2.23811454e-04,  8.34885850e-06, -3.18835664e-04, ...,\n",
       "         -1.77067595e-05, -1.10747904e-04,  1.35690134e-04],\n",
       "        [-6.37827441e-04,  9.74682553e-05, -3.26794718e-04, ...,\n",
       "         -3.90538451e-04, -5.97162929e-04, -1.02126345e-04],\n",
       "        [-1.13865186e-03,  2.56281986e-04, -2.16956876e-04, ...,\n",
       "         -1.04868878e-03, -1.28518464e-03, -5.06953162e-04],\n",
       "        ...,\n",
       "        [-2.72787292e-03, -2.72109709e-03,  1.45843462e-03, ...,\n",
       "         -9.62721359e-04, -2.41024466e-03, -4.23983322e-04],\n",
       "        [-3.03340517e-03, -3.42185399e-03,  1.93895237e-03, ...,\n",
       "         -9.91003588e-04, -2.42205570e-03, -1.40360949e-04],\n",
       "        [-3.30862147e-03, -4.07852000e-03,  2.36367458e-03, ...,\n",
       "         -1.04492379e-03, -2.43023899e-03,  1.40475779e-04]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in dataset.take(1): \n",
    "    break\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28535ad9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  12289024  \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  25174016  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  33562624  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  24590049  \n",
      "=================================================================\n",
      "Total params: 95,615,713\n",
      "Trainable params: 95,615,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14b40548",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "497/497 [==============================] - 288s 574ms/step - loss: 3.2555 - val_loss: 2.8688\n",
      "Epoch 2/10\n",
      "497/497 [==============================] - 286s 575ms/step - loss: 2.6864 - val_loss: 2.6026\n",
      "Epoch 3/10\n",
      "497/497 [==============================] - 285s 574ms/step - loss: 2.3434 - val_loss: 2.4196\n",
      "Epoch 4/10\n",
      "497/497 [==============================] - 286s 575ms/step - loss: 2.0133 - val_loss: 2.2826\n",
      "Epoch 5/10\n",
      "497/497 [==============================] - 286s 575ms/step - loss: 1.7085 - val_loss: 2.1948\n",
      "Epoch 6/10\n",
      "497/497 [==============================] - 286s 575ms/step - loss: 1.4482 - val_loss: 2.1424\n",
      "Epoch 7/10\n",
      "497/497 [==============================] - 286s 575ms/step - loss: 1.2448 - val_loss: 2.1293\n",
      "Epoch 8/10\n",
      "497/497 [==============================] - 286s 575ms/step - loss: 1.1022 - val_loss: 2.1393\n",
      "Epoch 9/10\n",
      "497/497 [==============================] - 286s 575ms/step - loss: 1.0196 - val_loss: 2.1600\n",
      "Epoch 10/10\n",
      "497/497 [==============================] - 286s 575ms/step - loss: 0.9771 - val_loss: 2.1826\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f146ea22c40>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#모델 학습하기\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "\n",
    "model.fit(enc_train, \n",
    "          dec_train, \n",
    "          epochs= 10,\n",
    "          batch_size=256,\n",
    "          validation_data=(enc_val, dec_val),\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98811783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 입력받은 init_sentence를 텐서로 변환합니다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 루프를 돌면서 init_sentence에 살을 붙여나갑니다.\n",
    "    while True:\n",
    "        predict = model(test_tensor)   \n",
    "        # 마지막 단어가 새롭게 예측한 단어입니다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   \n",
    "\n",
    "        # 새롭게 예측한 단어를 init_sentence의 뒤에 붙여 줍니다. \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 모델이 <end>를 예측했거나, max_len에 도달하면  while 루프를 탈출해 모델 작동을 멈춥니다.\n",
    "        if predict_word.numpy()[0] == end_token: \n",
    "            break\n",
    "        if test_tensor.shape[1] >= max_len: \n",
    "            break\n",
    "\n",
    "    generated = \"\"\n",
    "    # word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 최종적으로 모델이 생성한 자연어 문장입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75979043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you <end> '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#문장 생성 함수 실행\n",
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cda13e",
   "metadata": {},
   "source": [
    "## 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d86c1c",
   "metadata": {},
   "source": [
    "이번 프로젝트는 내가 가장 노력하고 시간도 많이 투자했다고 생각한다. validation loss를 낮추는 목표만을 보고 달려왔지만 낮추려고 정말 여러 가지 시도를 해봤다.\n",
    "또한, 어떤 부분을 고쳐보고 하이퍼파라미터를 바꿔보고 하는지 정하기 위해 코드를 하나씩 분석도 같이 수업 시간 외에도 그루들과 얘기해 보면서 풀어나갔다.\n",
    "사실은 처음부터 이런 식으로 공부해 왔어야 했나 라는 상실감도 있었지만 그래도 이렇게 마무리할 수 있어서 기쁘다.\n",
    "\n",
    "내가 일주일 내내 프로젝트를 붙잡은 적이 오랜만이라 다 기억은 안 나지만 시도해 본 것을 정리한다면 아래와 같다.\n",
    "최종 결과는 모두 validation loss 값을 말한다.\n",
    "\n",
    "1. 임베딩 사이즈와 히든 사이즈를 증가시켜 봤는데 오히려 증가\n",
    "2. max_len이 너무 짧다고 생각해 증가시켜 봤는데 오히려 증가\n",
    "3. drop out을 추가해 봤지만 증가\n",
    "4. 규제를 learning_rate=0.1로 추가해 봤지만, 이것 또한 증가\n",
    "5. dataset을 train이랑 validation이랑 다 나눠서 정해봤지만, 이것 또한 증가\n",
    "\n",
    "정말 뭐든 과정이 loss는 감소하지만, validation loss는 증가해 버렸다. 증가하는 이유는 사실 다양하지만, 전처리가 잘못된 건지 데이터가 적어인지 감소하지 않더라.\n",
    "심지어, 에폭이 나는 평균 한 시간 반이나 소요 돼버리는 바람에 정말 긴 시간 인내심을 기를 수 있었다.\n",
    "\n",
    "최종적으로는, 데이터셋을 나눌 때 random state를 정해주고 학습할 때 배치 크기를 추가해 실행해 봤지만 과대적합은 피할 수 없었다.\n",
    "그래도 validation_loss 값을 2.2 이하로 만들었고 문장도 해석할 수 있는 문장으로 만들어서 최종 제출하게 되었다.\n",
    "추가로, validation_loss가 2.2보다 낮더라도 문장이 이상하게 나오는 경우도 있다고 들었는데 그 이유는 잘 모르겠다.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
