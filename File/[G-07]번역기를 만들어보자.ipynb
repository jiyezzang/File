{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0131d40",
   "metadata": {},
   "source": [
    "## 번역기를 만들어보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b052c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ceea4e",
   "metadata": {},
   "source": [
    "### (1) 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56e29799",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 217975\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "      <th>cc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21361</th>\n",
       "      <td>Leave that to me.</td>\n",
       "      <td>Laisse-moi ça !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149102</th>\n",
       "      <td>He is an archeologist's assistant.</td>\n",
       "      <td>C'est un assistant en archéologie.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131309</th>\n",
       "      <td>It's awfully cold this evening.</td>\n",
       "      <td>Comment on se les pèle ce soir.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10372</th>\n",
       "      <td>Don't be hasty.</td>\n",
       "      <td>Ne te précipite pas.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123424</th>\n",
       "      <td>In what country were you born?</td>\n",
       "      <td>Dans quel pays es-tu né ?</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       eng  \\\n",
       "21361                    Leave that to me.   \n",
       "149102  He is an archeologist's assistant.   \n",
       "131309     It's awfully cold this evening.   \n",
       "10372                      Don't be hasty.   \n",
       "123424      In what country were you born?   \n",
       "\n",
       "                                       fra  \\\n",
       "21361                      Laisse-moi ça !   \n",
       "149102  C'est un assistant en archéologie.   \n",
       "131309     Comment on se les pèle ce soir.   \n",
       "10372                 Ne te précipite pas.   \n",
       "123424           Dans quel pays es-tu né ?   \n",
       "\n",
       "                                                       cc  \n",
       "21361   CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
       "149102  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "131309  CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
       "10372   CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "123424  CC-BY 2.0 (France) Attribution: tatoeba.org #9...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "file_path = os.getenv('HOME')+'/aiffel/translator_seq2seq/data/fra.txt'\n",
    "lines = pd.read_csv(file_path, names=['eng', 'fra', 'cc'], sep='\\t')\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "lines.sample(5) #샘플 5개 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704b595a",
   "metadata": {},
   "source": [
    "세번째 열은 불필요하므로 제거하고, 훈련 데이터는 5만개의 샘플로 줄이겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6c21856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2473</th>\n",
       "      <td>Stop lying.</td>\n",
       "      <td>Arrêtez de mentir.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37551</th>\n",
       "      <td>Are things that bad?</td>\n",
       "      <td>Les choses sont-elles si mauvaises ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19398</th>\n",
       "      <td>He laughed at me.</td>\n",
       "      <td>Il me rit au nez.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15293</th>\n",
       "      <td>I prefer biking.</td>\n",
       "      <td>Je préfère faire du vélo.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12403</th>\n",
       "      <td>Stop sniffling.</td>\n",
       "      <td>Arrêtez de renifler.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        eng                                   fra\n",
       "2473            Stop lying.                    Arrêtez de mentir.\n",
       "37551  Are things that bad?  Les choses sont-elles si mauvaises ?\n",
       "19398     He laughed at me.                     Il me rit au nez.\n",
       "15293      I prefer biking.             Je préfère faire du vélo.\n",
       "12403       Stop sniffling.                  Arrêtez de renifler."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = lines[['eng', 'fra']][:50000] # 5만개 샘플 사용\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dff9dbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 50000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>Try some.</td>\n",
       "      <td>\\t Essaie. \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           eng            fra\n",
       "841  Try some.  \\t Essaie. \\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 시작 토큰과 종료 토큰 추가\n",
    "sos_token = '\\t'\n",
    "eos_token = '\\n'\n",
    "lines.fra = lines.fra.apply(lambda x : '\\t '+ x + ' \\n')\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "lines.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59d43130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[19, 4, 7], [19, 4, 7], [19, 4, 7]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_tokenizer = Tokenizer(char_level=True)   # 문자 단위로 Tokenizer를 생성합니다. \n",
    "eng_tokenizer.fit_on_texts(lines.eng)               # 50000개의 행을 가진 eng의 각 행에 토큰화를 수행\n",
    "input_text = eng_tokenizer.texts_to_sequences(lines.eng)    # 단어를 숫자값 인덱스로 변환하여 저장\n",
    "input_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df0cf9fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[10, 1, 19, 5, 1, 31, 1, 11],\n",
       " [10, 1, 15, 5, 12, 16, 29, 2, 14, 1, 11],\n",
       " [10, 1, 2, 7, 1, 12, 9, 8, 4, 2, 1, 31, 1, 11]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fra_tokenizer = Tokenizer(char_level=True)   # 문자 단위로 Tokenizer를 생성합니다. \n",
    "fra_tokenizer.fit_on_texts(lines.fra)                 # 50000개의 행을 가진 fra의 각 행에 토큰화를 수행\n",
    "target_text = fra_tokenizer.texts_to_sequences(lines.fra)     # 단어를 숫자값 인덱스로 변환하여 저장\n",
    "target_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d550ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어장의 크기 : 52\n",
      "프랑스어 단어장의 크기 : 73\n"
     ]
    }
   ],
   "source": [
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58a8e355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 시퀀스의 최대 길이 21\n",
      "프랑스어 시퀀스의 최대 길이 69\n"
     ]
    }
   ],
   "source": [
    "max_eng_seq_len = max([len(line) for line in input_text])\n",
    "max_fra_seq_len = max([len(line) for line in target_text])\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "769718cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 50000\n",
      "영어 단어장의 크기 : 52\n",
      "프랑스어 단어장의 크기 : 73\n",
      "영어 시퀀스의 최대 길이 21\n",
      "프랑스어 시퀀스의 최대 길이 69\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플의 수 :',len(lines))\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3c2bbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = input_text\n",
    "# 종료 토큰 제거\n",
    "decoder_input = [[ char for char in line if char != fra_tokenizer.word_index[eos_token] ] for line in target_text] \n",
    "# 시작 토큰 제거\n",
    "decoder_target = [[ char for char in line if char != fra_tokenizer.word_index[sos_token] ] for line in target_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95e5621e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10, 1, 19, 5, 1, 31, 1], [10, 1, 15, 5, 12, 16, 29, 2, 14, 1], [10, 1, 2, 7, 1, 12, 9, 8, 4, 2, 1, 31, 1]]\n",
      "[[1, 19, 5, 1, 31, 1, 11], [1, 15, 5, 12, 16, 29, 2, 14, 1, 11], [1, 2, 7, 1, 12, 9, 8, 4, 2, 1, 31, 1, 11]]\n"
     ]
    }
   ],
   "source": [
    "print(decoder_input[:3])\n",
    "print(decoder_target[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bce2a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 데이터의 크기(shape) : (50000, 21)\n",
      "프랑스어 입력데이터의 크기(shape) : (50000, 69)\n",
      "프랑스어 출력데이터의 크기(shape) : (50000, 69)\n"
     ]
    }
   ],
   "source": [
    "encoder_input = pad_sequences(encoder_input, maxlen = max_eng_seq_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen = max_fra_seq_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen = max_fra_seq_len, padding='post')\n",
    "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e6a2773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19  4  7  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b38c726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 데이터의 크기(shape) : (50000, 21, 52)\n",
      "프랑스어 입력데이터의 크기(shape) : (50000, 69, 73)\n",
      "프랑스어 출력데이터의 크기(shape) : (50000, 69, 73)\n"
     ]
    }
   ],
   "source": [
    "encoder_input = to_categorical(encoder_input)\n",
    "decoder_input = to_categorical(decoder_input)\n",
    "decoder_target = to_categorical(decoder_target)\n",
    "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5717ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 학습데이터의 크기(shape) : (50000, 21, 52)\n",
      "프랑스어 학습 입력데이터의 크기(shape) : (50000, 69, 73)\n",
      "프랑스어 학습 출력데이터의 크기(shape) : (50000, 69, 73)\n"
     ]
    }
   ],
   "source": [
    "n_of_val = 3000\n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('영어 학습데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 학습 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 학습 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b34640",
   "metadata": {},
   "source": [
    "### (2) 모델 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dba1faf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "print('⏳')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5af7c84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 텐서 생성.\n",
    "encoder_inputs = Input(shape=(None, eng_vocab_size))\n",
    "# hidden size가 256인 인코더의 LSTM 셀 생성\n",
    "encoder_lstm = LSTM(units = 256, return_state = True)\n",
    "# 디코더로 전달할 hidden state, cell state를 리턴. encoder_outputs은 여기서는 불필요.\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "# hidden state와 cell state를 다음 time step으로 전달하기 위해서 별도 저장.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93687fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 텐서 생성.\n",
    "decoder_inputs = Input(shape=(None, fra_vocab_size))\n",
    "# hidden size가 256인 인코더의 LSTM 셀 생성\n",
    "decoder_lstm = LSTM(units = 256, return_sequences = True, return_state=True)\n",
    "# decoder_outputs는 모든 time step의 hidden state\n",
    "decoder_outputs, _, _= decoder_lstm(decoder_inputs, initial_state = encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d914e42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_softmax_layer = Dense(fra_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1155ae89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, 52)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None, 73)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 256), (None, 316416      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  337920      input_2[0][0]                    \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 73)     18761       lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 673,097\n",
      "Trainable params: 673,097\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21ddfbd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "368/368 [==============================] - 11s 20ms/step - loss: 0.9393 - val_loss: 0.7992\n",
      "Epoch 2/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.5851 - val_loss: 0.6819\n",
      "Epoch 3/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.4914 - val_loss: 0.5934\n",
      "Epoch 4/50\n",
      "368/368 [==============================] - 6s 18ms/step - loss: 0.4312 - val_loss: 0.5410\n",
      "Epoch 5/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.3903 - val_loss: 0.5012\n",
      "Epoch 6/50\n",
      "368/368 [==============================] - 6s 18ms/step - loss: 0.3607 - val_loss: 0.4728\n",
      "Epoch 7/50\n",
      "368/368 [==============================] - 6s 18ms/step - loss: 0.3383 - val_loss: 0.4455\n",
      "Epoch 8/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.3205 - val_loss: 0.4362\n",
      "Epoch 9/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.3056 - val_loss: 0.4175\n",
      "Epoch 10/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.2933 - val_loss: 0.4080\n",
      "Epoch 11/50\n",
      "368/368 [==============================] - 6s 18ms/step - loss: 0.2832 - val_loss: 0.3995\n",
      "Epoch 12/50\n",
      "368/368 [==============================] - 6s 18ms/step - loss: 0.2734 - val_loss: 0.4000\n",
      "Epoch 13/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.2657 - val_loss: 0.3920\n",
      "Epoch 14/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.2579 - val_loss: 0.3821\n",
      "Epoch 15/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.2511 - val_loss: 0.3800\n",
      "Epoch 16/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.2449 - val_loss: 0.3795\n",
      "Epoch 17/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.2392 - val_loss: 0.3789\n",
      "Epoch 18/50\n",
      "368/368 [==============================] - 6s 18ms/step - loss: 0.2338 - val_loss: 0.3787\n",
      "Epoch 19/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.2287 - val_loss: 0.3815\n",
      "Epoch 20/50\n",
      "368/368 [==============================] - 6s 18ms/step - loss: 0.2240 - val_loss: 0.3757\n",
      "Epoch 21/50\n",
      "368/368 [==============================] - 6s 18ms/step - loss: 0.2196 - val_loss: 0.3779\n",
      "Epoch 22/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.2153 - val_loss: 0.3747\n",
      "Epoch 23/50\n",
      "368/368 [==============================] - 6s 18ms/step - loss: 0.2113 - val_loss: 0.3748\n",
      "Epoch 24/50\n",
      "368/368 [==============================] - 6s 18ms/step - loss: 0.2074 - val_loss: 0.3729\n",
      "Epoch 25/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.2038 - val_loss: 0.3763\n",
      "Epoch 26/50\n",
      "368/368 [==============================] - 6s 18ms/step - loss: 0.2002 - val_loss: 0.3746\n",
      "Epoch 27/50\n",
      "368/368 [==============================] - 6s 18ms/step - loss: 0.1969 - val_loss: 0.3798\n",
      "Epoch 28/50\n",
      "368/368 [==============================] - 6s 18ms/step - loss: 0.1936 - val_loss: 0.3817\n",
      "Epoch 29/50\n",
      "368/368 [==============================] - 6s 18ms/step - loss: 0.1905 - val_loss: 0.3789\n",
      "Epoch 30/50\n",
      "368/368 [==============================] - 6s 18ms/step - loss: 0.1875 - val_loss: 0.3794\n",
      "Epoch 31/50\n",
      "368/368 [==============================] - 6s 18ms/step - loss: 0.1847 - val_loss: 0.3860\n",
      "Epoch 32/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1819 - val_loss: 0.3857\n",
      "Epoch 33/50\n",
      "368/368 [==============================] - 6s 18ms/step - loss: 0.1792 - val_loss: 0.3845\n",
      "Epoch 34/50\n",
      "368/368 [==============================] - 6s 18ms/step - loss: 0.1765 - val_loss: 0.3841\n",
      "Epoch 35/50\n",
      "368/368 [==============================] - 6s 18ms/step - loss: 0.1741 - val_loss: 0.3905\n",
      "Epoch 36/50\n",
      "368/368 [==============================] - 6s 18ms/step - loss: 0.1715 - val_loss: 0.3936\n",
      "Epoch 37/50\n",
      "368/368 [==============================] - 6s 18ms/step - loss: 0.1691 - val_loss: 0.3946\n",
      "Epoch 38/50\n",
      "368/368 [==============================] - 6s 18ms/step - loss: 0.1669 - val_loss: 0.3968\n",
      "Epoch 39/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.1646 - val_loss: 0.3991\n",
      "Epoch 40/50\n",
      "368/368 [==============================] - 6s 18ms/step - loss: 0.1626 - val_loss: 0.3997\n",
      "Epoch 41/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1603 - val_loss: 0.4038\n",
      "Epoch 42/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.1584 - val_loss: 0.4108\n",
      "Epoch 43/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1563 - val_loss: 0.4084\n",
      "Epoch 44/50\n",
      "368/368 [==============================] - 6s 18ms/step - loss: 0.1543 - val_loss: 0.4127\n",
      "Epoch 45/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.1525 - val_loss: 0.4153\n",
      "Epoch 46/50\n",
      "368/368 [==============================] - 6s 18ms/step - loss: 0.1508 - val_loss: 0.4176\n",
      "Epoch 47/50\n",
      "368/368 [==============================] - 6s 18ms/step - loss: 0.1490 - val_loss: 0.4189\n",
      "Epoch 48/50\n",
      "368/368 [==============================] - 6s 18ms/step - loss: 0.1472 - val_loss: 0.4203\n",
      "Epoch 49/50\n",
      "368/368 [==============================] - 6s 18ms/step - loss: 0.1457 - val_loss: 0.4268\n",
      "Epoch 50/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1440 - val_loss: 0.4243\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f75d798c0a0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
    "          validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "          batch_size=128, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9965ae4",
   "metadata": {},
   "source": [
    "###  (3) 모델 테스트하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c077db27",
   "metadata": {},
   "source": [
    "테스트 단계에서의 디코더의 동작 순서\n",
    " - 인코더에 입력 문장을 넣어 마지막 time step의 hidden, cell state를 얻는다.\n",
    " - <sos> 토큰인 '\\t'를 디코더에 입력한다.\n",
    " - 이전 time step의 출력층의 예측 결과를 현재 time step의 입력으로 한다.\n",
    " - 3을 반복하다가 <eos> 토큰인 '\\n'이 예측되면 이를 중단한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad1ae9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None, 52)]        0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  [(None, 256), (None, 256) 316416    \n",
      "=================================================================\n",
      "Total params: 316,416\n",
      "Trainable params: 316,416\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#인코더 정의\n",
    "encoder_model = Model(inputs = encoder_inputs, outputs = encoder_states)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e45773a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#디코더 정의\n",
    "# 이전 time step의 hidden state를 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "# 이전 time step의 cell state를 저장하는 텐서\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "# 이전 time step의 hidden state와 cell state를 하나의 변수에 저장\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# decoder_states_inputs를 현재 time step의 초기 상태로 사용.\n",
    "# 구체적인 동작 자체는 def decode_sequence()에 구현.\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state = decoder_states_inputs)\n",
    "# 현재 time step의 hidden state와 cell state를 하나의 변수에 저장.\n",
    "decoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44cbdfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, None, 73)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  337920      input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 73)     18761       lstm_1[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 356,681\n",
      "Trainable params: 356,681\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#디코더의 재출력 재설계\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "079820dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng2idx = eng_tokenizer.word_index\n",
    "fra2idx = fra_tokenizer.word_index\n",
    "idx2eng = eng_tokenizer.index_word\n",
    "idx2fra = fra_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e200b15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # <SOS>에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1, 1, fra_vocab_size))\n",
    "    target_seq[0, 0, fra2idx['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = idx2fra[sampled_token_index]\n",
    "\n",
    "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_fra_seq_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1, 1, fra_vocab_size))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "710387eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "입력 문장: Go.\n",
      "정답 문장:  Bouge ! \n",
      "번역기가 번역한 문장:  contrôle-toi. \n",
      "-----------------------------------\n",
      "입력 문장: Hello!\n",
      "정답 문장:  Bonjour ! \n",
      "번역기가 번역한 문장:  salut ! \n",
      "-----------------------------------\n",
      "입력 문장: Got it!\n",
      "정답 문장:  Compris ! \n",
      "번역기가 번역한 문장:  comparse-toi ! \n",
      "-----------------------------------\n",
      "입력 문장: Goodbye.\n",
      "정답 문장:  Au revoir. \n",
      "번역기가 번역한 문장:  au lait. \n",
      "-----------------------------------\n",
      "입력 문장: Hands off.\n",
      "정답 문장:  Pas touche ! \n",
      "번역기가 번역한 문장:  taissez-vous ! \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for seq_index in [3,50,100,300,1001]: # 입력 문장의 인덱스 (자유롭게 선택해 보세요)\n",
    "    input_seq = encoder_input[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', lines.eng[seq_index])\n",
    "    print('정답 문장:', lines.fra[seq_index][1:len(lines.fra[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n",
    "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1]) # '\\n'을 빼고 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15c75e2",
   "metadata": {},
   "source": [
    "## 프로젝트 : 단어 Level로 번역기 업그레이드하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786fe2b5",
   "metadata": {},
   "source": [
    "동일한 데이터셋을 사용하면서 글자 단위와는 다른 전처리와 to_categorical() 함수가 아닌 임베딩 층(Embedding layer)를 추가하여 단어 단위의 번역기를 완성시켜보겠습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658dee50",
   "metadata": {},
   "source": [
    "#### 주요 라이브러리 버전을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d4122330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf8bbb39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 217975\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "      <th>cc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>75641</th>\n",
       "      <td>Who are they talking to?</td>\n",
       "      <td>À qui sont-ils en train de parler ?</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140940</th>\n",
       "      <td>What problems did they run into?</td>\n",
       "      <td>Quels problèmes ont-elles rencontrés ?</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140155</th>\n",
       "      <td>Tom could've hurt himself today.</td>\n",
       "      <td>Tom aurait pu se blesser aujourd'hui.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32938</th>\n",
       "      <td>I tried not to cry.</td>\n",
       "      <td>J'ai essayé de ne pas pleurer.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189929</th>\n",
       "      <td>My name is known to everybody in my school.</td>\n",
       "      <td>Mon nom est connu de tous dans mon école.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                eng  \\\n",
       "75641                      Who are they talking to?   \n",
       "140940             What problems did they run into?   \n",
       "140155             Tom could've hurt himself today.   \n",
       "32938                           I tried not to cry.   \n",
       "189929  My name is known to everybody in my school.   \n",
       "\n",
       "                                              fra  \\\n",
       "75641         À qui sont-ils en train de parler ?   \n",
       "140940     Quels problèmes ont-elles rencontrés ?   \n",
       "140155      Tom aurait pu se blesser aujourd'hui.   \n",
       "32938              J'ai essayé de ne pas pleurer.   \n",
       "189929  Mon nom est connu de tous dans mon école.   \n",
       "\n",
       "                                                       cc  \n",
       "75641   CC-BY 2.0 (France) Attribution: tatoeba.org #5...  \n",
       "140940  CC-BY 2.0 (France) Attribution: tatoeba.org #6...  \n",
       "140155  CC-BY 2.0 (France) Attribution: tatoeba.org #4...  \n",
       "32938   CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "189929  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "file_path = os.getenv('HOME')+'/aiffel/translator_seq2seq/data/fra.txt'\n",
    "lines = pd.read_csv(file_path, names=['eng', 'fra', 'cc'], sep='\\t')\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "lines.sample(5) #샘플 5개 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "73f39e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6094</th>\n",
       "      <td>Thanks a lot.</td>\n",
       "      <td>Mille mercis.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18919</th>\n",
       "      <td>Do you like that?</td>\n",
       "      <td>Ça te plaît ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19272</th>\n",
       "      <td>Have some coffee.</td>\n",
       "      <td>Prends du café !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7006</th>\n",
       "      <td>Can you pitch?</td>\n",
       "      <td>Sais-tu lancer ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22893</th>\n",
       "      <td>Tom stepped away.</td>\n",
       "      <td>Tom démissionna.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     eng               fra\n",
       "6094       Thanks a lot.     Mille mercis.\n",
       "18919  Do you like that?     Ça te plaît ?\n",
       "19272  Have some coffee.  Prends du café !\n",
       "7006      Can you pitch?  Sais-tu lancer ?\n",
       "22893  Tom stepped away.  Tom démissionna."
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = lines[['eng', 'fra']][:33000] # 3만3천개 샘플 사용 \n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe7d337",
   "metadata": {},
   "source": [
    "3000개는 테스트 데이터로 분리하여 모델을 학습한 후에 번역을 테스트 하는 용도로 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d03d75",
   "metadata": {},
   "source": [
    "### Step 1. 정제, 정규화, 전처리 (영어, 프랑스어 모두!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e50d7be",
   "metadata": {},
   "source": [
    "1. 구두점(Punctuation)을 단어와 분리해주세요.\n",
    "2. 소문자로 바꿔주세요.\n",
    "3. 띄어쓰기 단위로 토큰화를 수행하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a23f016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[19, 4, 7], [19, 4, 7], [19, 4, 7]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_tokenizer = Tokenizer(char_level=True)   # 문자 단위로 Tokenizer를 생성합니다. \n",
    "eng_tokenizer.fit_on_texts(lines.eng)               # 33000개의 행을 가진 eng의 각 행에 토큰화를 수행\n",
    "input_text = eng_tokenizer.texts_to_sequences(lines.eng)    # 단어를 숫자값 인덱스로 변환하여 저장\n",
    "input_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "44cb6290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[17, 5, 1, 29], [13, 5, 10, 14, 26, 2, 11], [2, 7, 1, 10, 8, 9, 4, 2, 1, 29]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fra_tokenizer = Tokenizer(char_level=True)   # 문자 단위로 Tokenizer를 생성합니다. \n",
    "fra_tokenizer.fit_on_texts(lines.fra)                 # 33000개의 행을 가진 fra의 각 행에 토큰화를 수행\n",
    "target_text = fra_tokenizer.texts_to_sequences(lines.fra)     # 단어를 숫자값 인덱스로 변환하여 저장\n",
    "target_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9ebb74ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_lines(lines):\n",
    "    # 구두점(Punctuation)을 단어와 분리\n",
    "    lines['eng'] = lines['eng'].apply(lambda x: re.sub(r\"([.,!?])\", r\" \\1 \", x))\n",
    "    lines['fra'] = lines['fra'].apply(lambda x: re.sub(r\"([.,!?])\", r\" \\1 \", x))\n",
    "\n",
    "    # 소문자로 변환\n",
    "    lines['eng'] = lines['eng'].apply(lambda x: x.lower())\n",
    "    lines['fra'] = lines['fra'].apply(lambda x: x.lower())\n",
    "\n",
    "    # 띄어쓰기 단위로 토큰화\n",
    "    lines['eng'] = lines['eng'].apply(lambda x: x.split())\n",
    "    lines['fra'] = lines['fra'].apply(lambda x: x.split())\n",
    "\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "49c6fe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 참고\n",
    "#import os, re\n",
    "\n",
    "# def preprocess_sentence(sentence):\n",
    "# sentence = sentence.lower().strip() # 1. 소문자로, 공백 없애기\n",
    "# sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2. 특수문자 양쪽에 공백\n",
    "# sentence = sentence.strip() # 5. 양쪽 공백 지우기. 1번에서도 공백 없애기 했는데 두번째이네\n",
    "# sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3. 여러개 공백은 공백 하나로\n",
    "\n",
    "# return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "713e8d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             eng                                 fra\n",
      "29866  [where, do, we, start, ?]            [où, commençons-nous, ?]\n",
      "12658       [they're, trying, .]      [elles, font, des, efforts, .]\n",
      "16382    [my, hair, is, gray, .]       [mes, cheveux, sont, gris, .]\n",
      "23998   [you, need, to, stop, .]  [il, faut, que, vous, arrêtiez, .]\n",
      "24068    [you're, a, monster, .]        [vous, êtes, un, monstre, .]\n"
     ]
    }
   ],
   "source": [
    "# lines 데이터프레임에 작업을 수행\n",
    "lines = preprocess_lines(lines[['eng', 'fra']][:33000])\n",
    "\n",
    "# 샘플 출력\n",
    "sample = lines.sample(5)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9713af16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[go, .]</td>\n",
       "      <td>[va, !]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[go, .]</td>\n",
       "      <td>[marche, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[go, .]</td>\n",
       "      <td>[en, route, !]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[go, .]</td>\n",
       "      <td>[bouge, !]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[hi, .]</td>\n",
       "      <td>[salut, !]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32990</th>\n",
       "      <td>[i, want, to, kiss, tom, .]</td>\n",
       "      <td>[je, veux, embrasser, tom, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32991</th>\n",
       "      <td>[i, want, to, kiss, you, .]</td>\n",
       "      <td>[je, veux, t'embrasser, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32992</th>\n",
       "      <td>[i, want, to, kiss, you, .]</td>\n",
       "      <td>[je, veux, vous, embrasser, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32993</th>\n",
       "      <td>[i, want, to, know, now, .]</td>\n",
       "      <td>[je, veux, le, savoir, maintenant, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32994</th>\n",
       "      <td>[i, want, to, know, why, .]</td>\n",
       "      <td>[je, veux, savoir, pourquoi, .]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32995 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               eng                                    fra\n",
       "0                          [go, .]                                [va, !]\n",
       "1                          [go, .]                            [marche, .]\n",
       "2                          [go, .]                         [en, route, !]\n",
       "3                          [go, .]                             [bouge, !]\n",
       "4                          [hi, .]                             [salut, !]\n",
       "...                            ...                                    ...\n",
       "32990  [i, want, to, kiss, tom, .]          [je, veux, embrasser, tom, .]\n",
       "32991  [i, want, to, kiss, you, .]             [je, veux, t'embrasser, .]\n",
       "32992  [i, want, to, kiss, you, .]         [je, veux, vous, embrasser, .]\n",
       "32993  [i, want, to, know, now, .]  [je, veux, le, savoir, maintenant, .]\n",
       "32994  [i, want, to, know, why, .]        [je, veux, savoir, pourquoi, .]\n",
       "\n",
       "[32995 rows x 2 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[['eng', 'fra']][:-5] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0787d85",
   "metadata": {},
   "source": [
    "### Step 2. 디코더의 문장에 시작 토큰과 종료 토큰을 넣어주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6a6eb607",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수: 33000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7915</th>\n",
       "      <td>[i, was, at, home, .]</td>\n",
       "      <td>\\tj'étais chez moi . \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15712</th>\n",
       "      <td>[i'm, not, a, thief, .]</td>\n",
       "      <td>\\tje ne suis pas une voleuse . \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22720</th>\n",
       "      <td>[tom, is, uninsured, .]</td>\n",
       "      <td>\\ttom n'est pas assuré . \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13529</th>\n",
       "      <td>[yes, ,, we, can, go, .]</td>\n",
       "      <td>\\toui , nous pouvons partir . \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5344</th>\n",
       "      <td>[i'll, get, one, .]</td>\n",
       "      <td>\\tje vais en avoir un . \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            eng                                fra\n",
       "7915      [i, was, at, home, .]            \\tj'étais chez moi . \\n\n",
       "15712   [i'm, not, a, thief, .]  \\tje ne suis pas une voleuse . \\n\n",
       "22720   [tom, is, uninsured, .]        \\ttom n'est pas assuré . \\n\n",
       "13529  [yes, ,, we, can, go, .]   \\toui , nous pouvons partir . \\n\n",
       "5344        [i'll, get, one, .]         \\tje vais en avoir un . \\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 시작 토큰과 종료 토큰 추가\n",
    "sos_token = '\\t'\n",
    "eos_token = '\\n'\n",
    "lines.fra = lines.fra.apply(lambda x: '\\t' + ' '.join(x) + ' \\n')  # 리스트를 문자열로 변환하여 연결\n",
    "\n",
    "print('전체 샘플의 수:', len(lines))\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "29870334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[go, .]</td>\n",
       "      <td>\\tva ! \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[go, .]</td>\n",
       "      <td>\\tmarche . \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[go, .]</td>\n",
       "      <td>\\ten route ! \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[go, .]</td>\n",
       "      <td>\\tbouge ! \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[hi, .]</td>\n",
       "      <td>\\tsalut ! \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32990</th>\n",
       "      <td>[i, want, to, kiss, tom, .]</td>\n",
       "      <td>\\tje veux embrasser tom . \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32991</th>\n",
       "      <td>[i, want, to, kiss, you, .]</td>\n",
       "      <td>\\tje veux t'embrasser . \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32992</th>\n",
       "      <td>[i, want, to, kiss, you, .]</td>\n",
       "      <td>\\tje veux vous embrasser . \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32993</th>\n",
       "      <td>[i, want, to, know, now, .]</td>\n",
       "      <td>\\tje veux le savoir maintenant . \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32994</th>\n",
       "      <td>[i, want, to, know, why, .]</td>\n",
       "      <td>\\tje veux savoir pourquoi . \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32995 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               eng                                  fra\n",
       "0                          [go, .]                            \\tva ! \\n\n",
       "1                          [go, .]                        \\tmarche . \\n\n",
       "2                          [go, .]                      \\ten route ! \\n\n",
       "3                          [go, .]                         \\tbouge ! \\n\n",
       "4                          [hi, .]                         \\tsalut ! \\n\n",
       "...                            ...                                  ...\n",
       "32990  [i, want, to, kiss, tom, .]         \\tje veux embrasser tom . \\n\n",
       "32991  [i, want, to, kiss, you, .]           \\tje veux t'embrasser . \\n\n",
       "32992  [i, want, to, kiss, you, .]        \\tje veux vous embrasser . \\n\n",
       "32993  [i, want, to, know, now, .]  \\tje veux le savoir maintenant . \\n\n",
       "32994  [i, want, to, know, why, .]       \\tje veux savoir pourquoi . \\n\n",
       "\n",
       "[32995 rows x 2 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[['eng', 'fra']][:-5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841bf20f",
   "metadata": {},
   "source": [
    "### Step 3. 케라스의 토크나이저로 텍스트를 숫자로 바꿔보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0e2be8a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[24, 1], [24, 1], [24, 1], [24, 1], [769, 1]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_tokenizer = Tokenizer(char_level=True)   # 문자 단위로 Tokenizer를 생성합니다. \n",
    "eng_tokenizer.fit_on_texts(lines.eng)               # 33000개의 행을 가진 eng의 각 행에 토큰화를 수행\n",
    "input_text = eng_tokenizer.texts_to_sequences(lines.eng)    # 단어를 숫자값 인덱스로 변환하여 저장\n",
    "input_text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8be0fc0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[9, 19, 5, 1, 31, 1, 10],\n",
       " [9, 15, 5, 12, 16, 28, 2, 1, 13, 1, 10],\n",
       " [9, 2, 7, 1, 12, 8, 11, 4, 2, 1, 31, 1, 10],\n",
       " [9, 26, 8, 11, 27, 2, 1, 31, 1, 10],\n",
       " [9, 3, 5, 14, 11, 4, 1, 31, 1, 10]]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fra_tokenizer = Tokenizer(char_level=True)   # 문자 단위로 Tokenizer를 생성합니다. \n",
    "fra_tokenizer.fit_on_texts(lines.fra)                 # 33000개의 행을 가진 fra의 각 행에 토큰화를 수행\n",
    "target_text = fra_tokenizer.texts_to_sequences(lines.fra)     # 단어를 숫자값 인덱스로 변환하여 저장\n",
    "target_text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ef9ac509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어장의 크기 : 4648\n",
      "프랑스어 단어장의 크기 : 70\n"
     ]
    }
   ],
   "source": [
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2f926487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 시퀀스의 최대 길이 8\n",
      "프랑스어 시퀀스의 최대 길이 61\n"
     ]
    }
   ],
   "source": [
    "max_eng_seq_len = max([len(line) for line in input_text])\n",
    "max_fra_seq_len = max([len(line) for line in target_text])\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "89003ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 33000\n",
      "영어 단어장의 크기 : 4648\n",
      "프랑스어 단어장의 크기 : 70\n",
      "영어 시퀀스의 최대 길이 8\n",
      "프랑스어 시퀀스의 최대 길이 61\n"
     ]
    }
   ],
   "source": [
    "# 데이터 최종확인\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "108bd81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = input_text\n",
    "# 종료 토큰 제거\n",
    "decoder_input = [[ char for char in line if char != fra_tokenizer.word_index[eos_token] ] for line in target_text] \n",
    "# 시작 토큰 제거\n",
    "decoder_target = [[ char for char in line if char != fra_tokenizer.word_index[sos_token] ] for line in target_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d4a886f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9, 19, 5, 1, 31, 1], [9, 15, 5, 12, 16, 28, 2, 1, 13, 1], [9, 2, 7, 1, 12, 8, 11, 4, 2, 1, 31, 1]]\n",
      "[[19, 5, 1, 31, 1, 10], [15, 5, 12, 16, 28, 2, 1, 13, 1, 10], [2, 7, 1, 12, 8, 11, 4, 2, 1, 31, 1, 10]]\n"
     ]
    }
   ],
   "source": [
    "print(decoder_input[:3])\n",
    "print(decoder_target[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "47691091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 데이터의 크기(shape) : (33000, 8)\n",
      "프랑스어 입력데이터의 크기(shape) : (33000, 61)\n",
      "프랑스어 출력데이터의 크기(shape) : (33000, 61)\n"
     ]
    }
   ],
   "source": [
    "encoder_input = pad_sequences(encoder_input, maxlen = max_eng_seq_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen = max_fra_seq_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen = max_fra_seq_len, padding='post')\n",
    "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "104840c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24  1  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6f1d5f29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 학습데이터의 크기(shape) : (33000, 8)\n",
      "프랑스어 학습 입력데이터의 크기(shape) : (33000, 61)\n",
      "프랑스어 학습 출력데이터의 크기(shape) : (33000, 61)\n"
     ]
    }
   ],
   "source": [
    "n_of_test = 3000\n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_test]\n",
    "decoder_input_train = decoder_input[:-n_of_test]\n",
    "decoder_target_train = decoder_target[:-n_of_test]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_test:]\n",
    "decoder_input_test = decoder_input[-n_of_test:]\n",
    "decoder_target_test = decoder_target[-n_of_test:]\n",
    "\n",
    "print('영어 학습데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 학습 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 학습 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f92f52c",
   "metadata": {},
   "source": [
    "### Step 4. 임베딩 층(Embedding layer) 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0befbfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c51c5e",
   "metadata": {},
   "source": [
    "주의할 점은 인코더와 디코더의 임베딩 층은 서로 다른 임베딩 층을 사용해야 하지만, 디코더의 훈련 과정과 테스트 과정(예측 과정)에서의 임베딩 층은 동일해야 한다는 것입니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "68428848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.layers import Input, Embedding, Masking\n",
    "\n",
    "# vocab_size = 30000\n",
    "# output_dim = 128\n",
    "\n",
    "# # 인코더에서 사용할 임베딩 층 사용 예시\n",
    "# encoder_inputs = Input(shape=(None,))\n",
    "# enc_emb =  Embedding(vocab_size, output_dim)(encoder_inputs)\n",
    "# encoder_lstm = LSTM(units=256, return_state=True)\n",
    "# encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f8a700bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, Masking\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "vocab_size = 30000\n",
    "output_dim = 128\n",
    "\n",
    "# Define the encoder model\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb = Embedding(vocab_size, output_dim)(encoder_inputs)\n",
    "encoder_lstm = LSTM(units=256, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "\n",
    "# Define the early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fec228b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련용 디코더 모델 정의\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb = Embedding(fra_vocab_size, 256)(decoder_inputs)\n",
    "dec_masking = Masking(mask_value=0.0)(dec_emb)\n",
    "decoder_lstm = LSTM(units=256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_masking, initial_state=[state_h, state_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "030444a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_dense = Dense(fra_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3ba99c",
   "metadata": {},
   "source": [
    "### Step 5. 모델 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d11949",
   "metadata": {},
   "source": [
    "#### Train 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c6b43f1b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 256)    17920       input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 128)    3840000     input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "masking (Masking)               (None, None, 256)    0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 256), (None, 394240      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 256),  525312      masking[0][0]                    \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 70)     17990       lstm_3[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 4,795,462\n",
      "Trainable params: 4,795,462\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d411e271",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "938/938 [==============================] - 17s 14ms/step - loss: 0.7478 - acc: 0.7932 - val_loss: 0.6447 - val_acc: 0.8065\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 0.4587 - acc: 0.8633 - val_loss: 0.5169 - val_acc: 0.8457\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 0.3744 - acc: 0.8882 - val_loss: 0.4473 - val_acc: 0.8670\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 12s 12ms/step - loss: 0.3256 - acc: 0.9022 - val_loss: 0.4121 - val_acc: 0.8771\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 0.2912 - acc: 0.9121 - val_loss: 0.3903 - val_acc: 0.8837\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 0.2646 - acc: 0.9197 - val_loss: 0.3735 - val_acc: 0.8890\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 0.2425 - acc: 0.9261 - val_loss: 0.3622 - val_acc: 0.8923\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 0.2239 - acc: 0.9315 - val_loss: 0.3531 - val_acc: 0.8951\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 0.2075 - acc: 0.9361 - val_loss: 0.3503 - val_acc: 0.8973\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 0.1925 - acc: 0.9406 - val_loss: 0.3462 - val_acc: 0.8991\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f75d3651460>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 훈련\n",
    "model.fit([encoder_input_train, decoder_input_train], decoder_target_train,\n",
    "          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "          callbacks=[early_stopping], epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2a789270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기서 자꾸 shape 에러가 나서 확인해보자.\n",
    "# 입력 데이터의 차원 확인\n",
    "# print('encoder_input_train shape:', encoder_input_train.shape)\n",
    "# print('decoder_input_train shape:', decoder_input_train.shape)\n",
    "# print('decoder_target_train shape:', decoder_target_train.shape)\n",
    "# print('encoder_input_test shape:', encoder_input_test.shape)\n",
    "# print('decoder_input_test shape:', decoder_input_test.shape)\n",
    "# print('decoder_target_test shape:', decoder_target_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "00244ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validation_loss')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# validation_loss를 시각화해보자\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 손실 함수 그래프\n",
    "plt.plot(model.history.history['val_loss'])\n",
    "\n",
    "plt.title('Validation_loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bc3237",
   "metadata": {},
   "source": [
    "#### Test 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5c760202",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 128)         3840000   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                [(None, 256), (None, 256) 394240    \n",
      "=================================================================\n",
      "Total params: 4,234,240\n",
      "Trainable params: 4,234,240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#인코더\n",
    "encoder_model = Model(inputs = encoder_inputs, outputs = [state_h, state_c])\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2bad18ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#디코더\n",
    "decoder_state_input_h = Input(shape=(256,)) #hidden state\n",
    "decoder_state_input_c = Input(shape=(256,)) #cell state\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "dec_emb2 = Embedding(fra_vocab_size, 256)(decoder_inputs)\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state = decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "\n",
    "decoder_outputs2 = decoder_softmax_layer(decoder_outputs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4923c294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 256)    17920       input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 256),  525312      embedding_2[0][0]                \n",
      "                                                                 input_7[0][0]                    \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 73)     18761       lstm_3[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 561,993\n",
      "Trainable params: 561,993\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs2] + decoder_states2)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9bf65324",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng2idx = eng_tokenizer.word_index\n",
    "fra2idx = fra_tokenizer.word_index\n",
    "idx2eng = eng_tokenizer.index_word\n",
    "idx2fra = fra_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929df76b",
   "metadata": {},
   "source": [
    "### Step 6. 모델 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "18ef550b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # 에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1,1)) \n",
    "    target_seq[0, 0] = fra2idx['\\t']\n",
    "    \n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = idx2fra[sampled_token_index]\n",
    "\n",
    "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # 에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_fra_seq_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장     \n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0be59a35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "입력 문장: ['run', '!']\n",
      "정답 문장: filez ! \n",
      "-----------------------------------\n",
      "입력 문장: ['wow', '!']\n",
      "정답 문장: wah ! \n",
      "-----------------------------------\n",
      "입력 문장: ['cheers', '!']\n",
      "정답 문장: tchin-tchin ! \n",
      "-----------------------------------\n",
      "입력 문장: ['go', 'back', '.']\n",
      "정답 문장: recule . \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def translate_sentences(sentences):\n",
    "    results = []\n",
    "    for seq_index in range(len(sentences)):\n",
    "        input_seq = encoder_input[seq_index: seq_index + 1]\n",
    "        decoded_sentence = decode_sequence(input_seq)\n",
    "        results.append(decoded_sentence[:len(decoded_sentence)-1]) # '\\n'을 빼고 결과에 추가\n",
    "\n",
    "    return results\n",
    "\n",
    "sentences = [10, 25, 90, 290] # 번역할 문장의 인덱스 (자유롭게 선택해 보세요)\n",
    "translations = translate_sentences(sentences)\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    seq_index = sentences[i]\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', lines.eng[seq_index])\n",
    "    print('정답 문장:', lines.fra[seq_index][1:len(lines.fra[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbceb8a7",
   "metadata": {},
   "source": [
    "## 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399fde14",
   "metadata": {},
   "source": [
    "이번 노드를 통해 NLP로 번역 과정을 두 가지 방법으로 구현해 보았다.\n",
    "첫 번째는 to_categorical() 함수로 정수형 레이블을 다중 클래스 분류 모델의 출력 형식에 맞게 원한 인코딩으로 된 이진 벡터를 사용했다. 두 번째 프로젝트에서는 임베딩 층을 추가하면서 텍스트 데이터를 고차원 벡터로 표현하였다. 결과는 파파고로 번역을 해보았을 때 100% 정확하진 않았지만 그래도 노력한 만큼 값이 나온 것 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365f85cf",
   "metadata": {},
   "source": [
    "https://wikidocs.net/22888"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f290370",
   "metadata": {},
   "source": [
    "https://sophihappy.tistory.com/2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289dbdd3",
   "metadata": {},
   "source": [
    "https://ponyozzang.tistory.com/335"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91947ac",
   "metadata": {},
   "source": [
    "https://tykimos.github.io/2018/09/14/ten-minute_introduction_to_sequence-to-sequence_learning_in_Keras/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
